{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### CONNECT TO ThalesStockPredictor SQLServer DB\n",
    "\n",
    "# Define connection string\n",
    "connStr = 'mssql+pyodbc://@MSI/ThalesStockPredictor?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(connStr)\n",
    "\n",
    "# Define the query\n",
    "query = \"\"\"\n",
    "SELECT * FROM vw_COMBINED_MODEL\n",
    "ORDER BY FK_DT_Date desc\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and assign the result to a pandas DataFrame\n",
    "Model_Data = pd.read_sql(query, engine)\n",
    "\n",
    "# Close the SQL Server Connection\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "Model_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original data frame for normalization\n",
    "Model_ML = Model_Data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'FK_DT_Date' to UNIX Epoch (numeric date)\n",
    "Model_ML['FK_DT_Date'] = pd.to_datetime(Model_ML['FK_DT_Date']).astype('int64') // 10**9\n",
    "\n",
    "# Remove rows with NA values (equivalent to na.omit in R)\n",
    "Model_ML.dropna(inplace=True)\n",
    "\n",
    "# Reset row names (equivalent to removing row names in R)\n",
    "Model_ML.reset_index(drop=True)\n",
    "\n",
    "Model_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 123\n",
    "\n",
    "# Proportion for training set\n",
    "train_prop = 0.8\n",
    "\n",
    "# Split data into training and initial test sets\n",
    "train_data, initial_test_data = train_test_split(Model_ML, test_size=1-train_prop, random_state=seed)\n",
    "\n",
    "# Proportion for validation set (from the initial test set)\n",
    "val_prop = 0.5\n",
    "\n",
    "# Split initial test data into validation and testing sets\n",
    "validate_data, test_data = train_test_split(initial_test_data, test_size=val_prop, random_state=seed)\n",
    "\n",
    "# Reset row indices\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "validate_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Un-Normalized train_data dataframe\n",
    "# Separate features and target variable\n",
    "features = train_data.drop(\"THA_NextDay_Close\", axis=1)  # Replace \"target_column\" with your target\n",
    "target = train_data[\"THA_NextDay_Close\"]\n",
    "\n",
    "# Create the Random Forest model for regression\n",
    "RFmodel = RandomForestRegressor(n_estimators=500)  # using 500 as there is a chance of overfitting n_estimators as needed\n",
    "\n",
    "# Train the model\n",
    "RFmodel.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importance = RFmodel.feature_importances_\n",
    "feature_names = features.columns\n",
    "sorted_idx = importance.argsort()[::-1]  # Sort features by importance (descending)\n",
    "sorted_features = feature_names[sorted_idx]\n",
    "sorted_importance = importance[sorted_idx]\n",
    "\n",
    "# print sorted importance\n",
    "print(\"Feature Importances (without scientific notation):\\n\")\n",
    "for feature, importance in zip(sorted_features, sorted_importance):\n",
    "    print(f\"{feature}: {importance:.8f}\") \n",
    "\n",
    "# Set a threshold for minimum importance (adjust as needed)\n",
    "importance_threshold = 0.00001500\n",
    "\n",
    "# Filter features and importance based on threshold\n",
    "filtered_features = sorted_features[sorted_importance >= importance_threshold]\n",
    "filtered_importance = sorted_importance[sorted_importance >= importance_threshold]\n",
    "\n",
    "# Print or visualize importance\n",
    "print(\"Sorted features by importance:\\n\", filtered_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to visualize un-normalized feature importance\n",
    "\n",
    "# Create the bar chart with filtered data\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(filtered_features, filtered_importance, color='skyblue')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance Visualization: Un-Normalized\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display most important feature at the top\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own `normalize()` function\n",
    "def normalize(x):\n",
    "    num = x - x.min()\n",
    "    denom = x.max() - x.min()\n",
    "    return num / denom\n",
    "\n",
    "# Create a list of column names to exclude\n",
    "exclude_columns = [\"FK_DT_Date\", \"THA_NextDay_Close\"]\n",
    "\n",
    "# Normalize columns in the training dataset (excluding specified columns)\n",
    "train_data.loc[:, ~train_data.columns.isin(exclude_columns)] = train_data.loc[:, ~train_data.columns.isin(exclude_columns)].apply(normalize)\n",
    "\n",
    "# Normalize columns in the validation dataset (excluding specified columns)\n",
    "validate_data.loc[:, ~validate_data.columns.isin(exclude_columns)] = validate_data.loc[:, ~validate_data.columns.isin(exclude_columns)].apply(normalize)\n",
    "\n",
    "# Normalize columns in the testing dataset (excluding specified columns)\n",
    "test_data.loc[:, ~test_data.columns.isin(exclude_columns)] = test_data.loc[:, ~test_data.columns.isin(exclude_columns)].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Normalized train_data dataframe\n",
    "# Separate features and target variable\n",
    "features_Norm = train_data.drop(\"THA_NextDay_Close\", axis=1)  \n",
    "target_Norm = train_data[\"THA_NextDay_Close\"]\n",
    "\n",
    "# Create the Random Forest model\n",
    "RFmodel_Norm = RandomForestRegressor(n_estimators=500)  # using 5400 as there is a chance of overfitting \n",
    "\n",
    "# Train the model\n",
    "RFmodel_Norm.fit(features_Norm, target_Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get feature importances\n",
    "importance_Norm = RFmodel_Norm.feature_importances_\n",
    "\n",
    "# Get feature importances\n",
    "importance_Norm = RFmodel_Norm.feature_importances_\n",
    "feature_names_Norm = features_Norm.columns\n",
    "sorted_idx_Norm = importance_Norm.argsort()[::-1]  # Sort features by importance (descending)\n",
    "sorted_features_Norm = feature_names_Norm[sorted_idx_Norm]\n",
    "sorted_importance_Norm = importance_Norm[sorted_idx_Norm]\n",
    "\n",
    "# print sorted importance\n",
    "print(\"Feature Importances (without scientific notation):\\n\")\n",
    "for feature_Norm, importance_Norm in zip(sorted_features_Norm, sorted_importance_Norm):\n",
    "    print(f\"{feature_Norm}: {importance_Norm:.8f}\") \n",
    "\n",
    "# Set a threshold for minimum importance (adjust as needed)\n",
    "importance_threshold = 0.00001500\n",
    "\n",
    "# Filter features and importance based on threshold\n",
    "filtered_features_Norm = sorted_features_Norm[sorted_importance_Norm >= importance_threshold]\n",
    "filtered_importance_Norm = sorted_importance_Norm[sorted_importance_Norm >= importance_threshold]\n",
    "\n",
    "# Print or visualize importance\n",
    "print(\"Sorted features by importance:\\n\", filtered_features_Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to visualize normalized feature importance\n",
    "\n",
    "# Create the bar chart with filtered data\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(filtered_features_Norm, filtered_importance_Norm, color='skyblue')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance Visualization: Normalized\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display most important feature at the top\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = train_data.drop(columns=['THA_NextDay_Close', 'FK_DT_Date'])\n",
    "y = train_data['THA_NextDay_Close']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models (excluding ARIMA)\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'k-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Multi-layer Perceptron': MLPRegressor(max_iter=1000),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'Support Vector Machine (Poly)': SVR(kernel='poly'),\n",
    "    'Support Vector Machine (Radial)': SVR(kernel='rbf'),\n",
    "    'Support Vector Machine (Linear)': SVR(kernel='linear')\n",
    "}\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def evaluate_model(model_name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"{model_name}: MSE = {mse:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "\n",
    "\n",
    "# Loop through models, train, and evaluate\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_model(model_name, model, X_test, y_test)\n",
    "\n",
    "\n",
    "# Example LSTM implementation (adjust hyperparameters as needed)\n",
    "# Define features (X) and target (y)\n",
    "X_train = train_data.drop('THA_NextDay_Close', axis=1)  # Drop target variable from features\n",
    "y_train = train_data['THA_NextDay_Close']\n",
    "\n",
    "# Get the number of features dynamically\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Define the LSTM model with dynamic input shape\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(n_features, 1)))  # Define input shape here\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "evaluate_model('LSTM', model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "# X = train_data.drop(columns=['THA_NextDay_Close', 'FK_DT_Date'])\n",
    "# y = train_data['THA_NextDay_Close']\n",
    "# Training Models:   0%|          | 0/10 [00:00<?, ?it/s]\n",
    "# Training Models:  10%|█         | 1/10 [00:23<03:33, 23.75s/it]\n",
    "# Random Forest: MSE = 1.7345, RMSE = 1.3170, MAE = 0.7861\n",
    "# Training Models:  20%|██        | 2/10 [00:23<01:19,  9.90s/it]\n",
    "# k-Nearest Neighbors: MSE = 1.9726, RMSE = 1.4045, MAE = 0.9244\n",
    "# Linear Regression: MSE = 1.6161, RMSE = 1.2713, MAE = 0.7594\n",
    "# Training Models:  40%|████      | 4/10 [00:34<00:41,  6.97s/it]\n",
    "# Gradient Boosting: MSE = 1.7452, RMSE = 1.3211, MAE = 0.8043\n",
    "# Training Models:  50%|█████     | 5/10 [00:34<00:24,  4.93s/it]\n",
    "# Decision Tree: MSE = 2.5008, RMSE = 1.5814, MAE = 1.0122\n",
    "# Training Models:  60%|██████    | 6/10 [00:42<00:23,  5.81s/it]\n",
    "# Multi-layer Perceptron: MSE = 2.1818, RMSE = 1.4771, MAE = 0.9099\n",
    "# Training Models:  70%|███████   | 7/10 [00:44<00:13,  4.67s/it]\n",
    "# XGBoost: MSE = 2.0275, RMSE = 1.4239, MAE = 0.8628\n",
    "# Training Models:  80%|████████  | 8/10 [00:45<00:07,  3.59s/it]\n",
    "# Support Vector Machine (Poly): MSE = 3.0388, RMSE = 1.7432, MAE = 1.1928\n",
    "# Training Models:  90%|█████████ | 9/10 [00:46<00:02,  2.81s/it]\n",
    "# Support Vector Machine (Radial): MSE = 5.2402, RMSE = 2.2892, MAE = 1.3272\n",
    "# Training Models: 100%|██████████| 10/10 [00:47<00:00,  4.75s/it]\n",
    "# Support Vector Machine (Linear): MSE = 2.1323, RMSE = 1.4602, MAE = 0.8534\n",
    "# LSTM: MSE = 86.3594, RMSE = 9.2930, MAE = 7.6911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (excluding target)\n",
    "features = [col for col in train_data.columns if col != \"THA_NextDay_Close\"]\n",
    "\n",
    "# Define target variable\n",
    "target = \"THA_NextDay_Close\"\n",
    "\n",
    "# Feature selection using F-value with classification approach\n",
    "selector = SelectKBest(f_classif, k=30)  # Choose top 10 features (adjust k as needed)\n",
    "selector.fit(train_data[features], train_data[target])\n",
    "\n",
    "# Get feature importances\n",
    "scores = selector.scores_\n",
    "\n",
    "# Get feature names with scores\n",
    "feature_scores = pd.DataFrame({'feature': features, 'score': scores})\n",
    "feature_scores = feature_scores.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Print top features\n",
    "print(\"Top Features based on F-value:\")\n",
    "print(feature_scores.head(30)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and scores\n",
    "features = feature_scores['feature'].to_numpy()\n",
    "scores = feature_scores['score'].to_numpy()\n",
    "\n",
    "# Sort together by scores (descending)\n",
    "sorted_data = pd.DataFrame({'feature': features, 'score': scores})\n",
    "sorted_data = sorted_data.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Extract sorted features and scores\n",
    "sorted_features = sorted_data['feature'].to_numpy()\n",
    "sorted_scores = sorted_data['score'].to_numpy()\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.plot(sorted_features, sorted_scores)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Feature Importance Score\")\n",
    "plt.title(\"Elbow Method for Feature Selection\")\n",
    "plt.xticks(rotation=45)  # Rotate feature names for readability\n",
    "\n",
    "# Optional: Add a grid for better visualization\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features starting with 'THA_'\n",
    "tha_features = [col for col in train_data.columns if col.startswith('THA_')]\n",
    "df_filtered = train_data[tha_features]\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_filtered.drop(columns=['THA_NextDay_Close'])\n",
    "y = df_filtered['THA_NextDay_Close']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models (excluding ARIMA)\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'k-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Multi-layer Perceptron': MLPRegressor(max_iter=1000),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'Support Vector Machine (Poly)': SVR(kernel='poly'),\n",
    "    'Support Vector Machine (Radial)': SVR(kernel='rbf'),\n",
    "    'Support Vector Machine (Linear)': SVR(kernel='linear')\n",
    "}\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def evaluate_model(model_name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"{model_name}: MSE = {mse:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "\n",
    "\n",
    "# Loop through models, train, and evaluate\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_model(model_name, model, X_test, y_test)\n",
    "\n",
    "\n",
    "# Example LSTM implementation (adjust hyperparameters as needed)\n",
    "# Define features (X) and target (y)\n",
    "X_train = df_filtered.drop('THA_NextDay_Close', axis=1)  # Drop target variable from features\n",
    "y_train = df_filtered['THA_NextDay_Close']\n",
    "\n",
    "# Get the number of features dynamically\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Define the LSTM model with dynamic input shape\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(n_features, 1)))  # Define input shape here\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "evaluate_model('LSTM', model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Features except FK_DT_Date\n",
    "# # Random Forest: MSE = 1.7035, RMSE = 1.3052, MAE = 0.7885\n",
    "# k-Nearest Neighbors: MSE = 2.2569, RMSE = 1.5023, MAE = 0.9749\n",
    "# Linear Regression: MSE = 1.6422, RMSE = 1.2815, MAE = 0.7633\n",
    "# Gradient Boosting: MSE = 1.7412, RMSE = 1.3195, MAE = 0.8053\n",
    "# Decision Tree: MSE = 2.5059, RMSE = 1.5830, MAE = 1.0459\n",
    "# Multi-layer Perceptron: MSE = 2.0624, RMSE = 1.4361, MAE = 0.8723\n",
    "# XGBoost: MSE = 1.9694, RMSE = 1.4034, MAE = 0.8629\n",
    "# Support Vector Machine (Poly): MSE = 14.9405, RMSE = 3.8653, MAE = 2.4632\n",
    "# Support Vector Machine (Radial): MSE = 3.7796, RMSE = 1.9441, MAE = 1.1282\n",
    "# Support Vector Machine (Linear): MSE = 2.2261, RMSE = 1.4920, MAE = 0.8778\n",
    "# LSTM: MSE = 2.5670, RMSE = 1.6022, MAE = 1.1268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features based on F-value\n",
    "top_features = list(feature_scores.head(30)['feature'])  # Adjust the number of features (k) as needed\n",
    "\n",
    "# Define features (X) for model training using selected features\n",
    "X = train_data[top_features]\n",
    "\n",
    "# Define target variable (y) for model training and evaluation\n",
    "y = train_data['THA_NextDay_Close']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models (excluding ARIMA)\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'k-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Multi-layer Perceptron': MLPRegressor(max_iter=1000),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'Support Vector Machine (Poly)': SVR(kernel='poly'),\n",
    "    'Support Vector Machine (Radial)': SVR(kernel='rbf'),\n",
    "    'Support Vector Machine (Linear)': SVR(kernel='linear')\n",
    "}\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def evaluate_model(model_name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"{model_name}: MSE = {mse:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "\n",
    "\n",
    "# Loop through models, train, and evaluate\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_model(model_name, model, X_test, y_test)\n",
    "\n",
    "\n",
    "# Example LSTM implementation (adjust hyperparameters as needed)\n",
    "# Define features (X) and target (y)\n",
    "X_train = train_data_filtered.drop('THA_NextDay_Close', axis=1)  # Drop target variable from features\n",
    "y_train = train_data_filtered['THA_NextDay_Close']\n",
    "\n",
    "# Get the number of features dynamically\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Define the LSTM model with dynamic input shape\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(n_features, 1)))  # Define input shape here\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "evaluate_model('LSTM', model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using top 30 Features based on Fscore\n",
    "# Random Forest: MSE = 1.7210, RMSE = 1.3119, MAE = 0.7858\n",
    "# k-Nearest Neighbors: MSE = 1.9707, RMSE = 1.4038, MAE = 0.8105\n",
    "# Linear Regression: MSE = 1.6077, RMSE = 1.2680, MAE = 0.7531\n",
    "# Gradient Boosting: MSE = 1.7464, RMSE = 1.3215, MAE = 0.8050\n",
    "# Decision Tree: MSE = 2.6304, RMSE = 1.6219, MAE = 1.0556\n",
    "# Multi-layer Perceptron: MSE = 2.1446, RMSE = 1.4644, MAE = 0.8781\n",
    "# XGBoost: MSE = 1.9582, RMSE = 1.3994, MAE = 0.8317\n",
    "# Support Vector Machine (Poly): MSE = 49.0350, RMSE = 7.0025, MAE = 5.6358\n",
    "# Support Vector Machine (Radial): MSE = 3.8672, RMSE = 1.9665, MAE = 1.1684\n",
    "# Support Vector Machine (Linear): MSE = 2.1745, RMSE = 1.4746, MAE = 0.8863\n",
    "# LSTM: MSE = 1069.6376, RMSE = 32.7053, MAE = 28.0060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) for model training using selected features\n",
    "X = train_data[filtered_features_Norm]\n",
    "\n",
    "# Define target variable (y) for model training and evaluation\n",
    "y = train_data['THA_NextDay_Close']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models (excluding ARIMA)\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'k-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Multi-layer Perceptron': MLPRegressor(max_iter=1000),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'Support Vector Machine (Poly)': SVR(kernel='poly'),\n",
    "    'Support Vector Machine (Radial)': SVR(kernel='rbf'),\n",
    "    'Support Vector Machine (Linear)': SVR(kernel='linear')\n",
    "}\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def evaluate_model(model_name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"{model_name}: MSE = {mse:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "\n",
    "\n",
    "# Loop through models, train, and evaluate\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    evaluate_model(model_name, model, X_test, y_test)\n",
    "\n",
    "# Example LSTM implementation (adjust hyperparameters as needed)\n",
    "# Reshape data for LSTM input\n",
    "X_train_LSTM = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_LSTM = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))  # Reshape test data as well\n",
    "\n",
    "# Define the LSTM model with dynamic input shape\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2])))  # Define input shape based on reshaped data\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train_LSTM, y_train, epochs=100, batch_size=32, validation_data=(X_test_LSTM, y_test))\n",
    "evaluate_model('LSTM', model, X_test_LSTM, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
